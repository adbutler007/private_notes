# Audio Summary App

A privacy-focused desktop application that captures audio (both microphone input and system output), transcribes it in real-time using **OpenAI Whisper**, and generates intelligent summaries using **Ollama** - all running locally on your computer.

## âœ¨ What's New

- ğŸš€ **Simplified Setup** - Uses Ollama (no manual model downloads!)
- âš¡ **Faster Installation** - Managed by `uv` for 10-100x faster dependency installation
- ğŸ¤– **Better Models** - Qwen3:1.7b for efficient, high-quality summaries
- ğŸ¯ **Auto-Download** - Whisper models download automatically on first use
- ğŸ’» **Native Support** - Runs on any modern Mac or Windows PC

## ğŸš€ Quick Start (5 minutes)

### 1. Install Ollama
```bash
# macOS
brew install ollama

# Windows: Download from https://ollama.com/download
# Linux
curl -fsSL https://ollama.com/install.sh | sh
```

### 2. Pull the Model
```bash
ollama pull qwen3:1.7b
```

### 3. Install Dependencies
```bash
# Using uv (recommended)
curl -LsSf https://astral.sh/uv/install.sh | sh
uv sync
source .venv/bin/activate

# Or using pip
pip install -r requirements.txt
```

### 4. Run It!
```bash
python main.py
```

**That's it!** Whisper will auto-download on first run (~140MB).

ğŸ“– **For detailed setup instructions, see [SETUP.md](SETUP.md)**

## ğŸ”’ Privacy First

- **No data saved to disk** except final summaries
- **No cloud services** - all processing happens locally
- Audio streams directly to transcription (never written to files)
- Transcripts kept only in RAM buffer
- You control what gets saved

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Microphone  â”‚â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                 â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”œâ”€â”€â”€â†’â”‚   Whisper    â”‚â”€â”€â”€â”€â”€â†’â”‚  Transcript  â”‚
â”‚System Audio â”‚â”€â”€â”˜    â”‚ (Speech-to-  â”‚      â”‚   Buffer     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚    Text)     â”‚      â”‚ (In Memory)  â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                    â”‚
                                                    â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚    Final     â”‚â—€â”€â”€â”€â”€â”€â”‚    Ollama    â”‚
                      â”‚   Summary    â”‚      â”‚(Map-Reduce   â”‚
                      â”‚  (Saved to   â”‚      â”‚ Summarizer)  â”‚
                      â”‚    Disk)     â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components

1. **Audio Capture Manager** ([audio_capture.py](audio_capture.py))
   - Captures microphone input (what you say)
   - Captures system audio output (what you hear)
   - Streams audio directly to processing pipeline
   - Audio data never touches disk

2. **Streaming Transcriber** ([transcriber.py](transcriber.py))
   - On-device speech-to-text using **OpenAI Whisper**
   - Processes audio chunks in real-time
   - Auto-downloads models on first use
   - Supports: tiny, base, small, medium, large

3. **Transcript Buffer** ([transcript_buffer.py](transcript_buffer.py))
   - Circular buffer in RAM (auto-discards old data)
   - Thread-safe transcript storage
   - Organizes segments into chunks for summarization
   - Completely in-memory, never touches disk

4. **Map-Reduce Summarizer** ([summarizer.py](summarizer.py))
   - Uses **Ollama** with **Qwen3:1.7b** for summarization
   - MAP Phase: Summarize individual time-based chunks (every 5 min)
   - REDUCE Phase: Combine chunk summaries into final comprehensive summary
   - All processing happens locally

## ğŸ“‹ Requirements

**Minimum:**
- Python 3.9+
- 4 GB RAM
- 3 GB disk space (for models)
- Dual-core CPU

**Recommended:**
- Python 3.11+
- 8 GB RAM
- 5 GB disk space
- Quad-core CPU

## ğŸ¯ Use Cases

- ğŸ“ Meeting notes and summaries
- ğŸ“ Lecture transcription
- ğŸ¤ Interview documentation
- ğŸ¥ Confidential discussions (HIPAA-friendly)
- ğŸ”¬ Research interviews
- â™¿ Accessibility support
- ğŸ““ Personal voice journaling

## âš™ï¸ Configuration

Edit [config.py](config.py) to customize:

```python
class Config:
    # Audio
    sample_rate: int = 16000  # 16kHz for speech
    channels: int = 1         # Mono

    # Transcription (Whisper)
    stt_model_path: str = "base"  # tiny, base, small, medium, large

    # Summarization (Ollama)
    llm_model_name: str = "qwen3:1.7b"  # Or: llama3.2:3b, phi3:3.8b

    # Summary
    summary_interval: int = 300  # Rolling summary every 5 minutes
    output_dir: str = "./summaries"
```

### Available Models

**Whisper** (auto-download):
- `tiny` (39 MB) - Fastest
- `base` (74 MB) - **Recommended**
- `small` (244 MB) - Better accuracy
- `medium` (769 MB) - High accuracy
- `large` (1.5 GB) - Best accuracy

**Ollama** (must pull separately):
- `qwen3:1.7b` (1.1 GB) - **Recommended** - Fast & efficient
- `llama3.2:3b` (2 GB) - Better quality
- `phi3:3.8b` (2.3 GB) - Good reasoning
- `gemma2:2b` (1.6 GB) - Fast alternative

```bash
ollama pull llama3.2:3b  # Example: use a different model
```

## ğŸ® Usage

### Start the app:
```bash
python main.py
```

### Commands:
- `start` - Begin recording and transcription
- `stop` - Stop recording and generate final summary
- `quit` - Exit the application

### Example Session:
```
$ python main.py
[STT] Loading Whisper model: base
[STT] Model loaded successfully
[LLM] Using Ollama model: qwen3:1.7b
[LLM] Model qwen3:1.7b is ready

Audio Summary App started.
Commands: start, stop, quit

> start
Recording started...

> stop
Recording stopped.
Generating final summary...
Summary saved to: ./summaries/summary_20250105_143022.txt

> quit
Goodbye!
```

## ğŸ§ª Demo Mode

Test without audio hardware:
```bash
python demo.py
```

This runs a simulation with mock data to verify the installation.

## ğŸ“ Project Structure

```
audio_summary_app/
â”œâ”€â”€ SETUP.md                # Quick setup guide (START HERE!)
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ pyproject.toml          # Modern Python project config
â”œâ”€â”€ requirements.txt        # Dependencies (pip)
â”œâ”€â”€ .python-version         # Python version for uv
â”œâ”€â”€ config.py               # Configuration settings
â”œâ”€â”€ main.py                 # Application entry point
â”œâ”€â”€ demo.py                 # Hardware-free demo
â”œâ”€â”€ audio_capture.py        # Audio input/output capture
â”œâ”€â”€ transcriber.py          # Whisper speech-to-text
â”œâ”€â”€ transcript_buffer.py    # In-memory transcript storage
â”œâ”€â”€ summarizer.py           # Ollama-based summarization
â”œâ”€â”€ ARCHITECTURE.md         # Detailed system design
â”œâ”€â”€ DATA_FLOW.md           # Privacy architecture
â”œâ”€â”€ INSTALL.md             # Platform-specific setup
â”œâ”€â”€ INSTALL_UV.md          # Installation with uv
â””â”€â”€ UV_MIGRATION.md        # Migration guide
```

## ğŸ”§ Troubleshooting

### Ollama not found
```bash
# Make sure Ollama is running
ollama list

# Start Ollama (usually auto-starts)
ollama serve
```

### Audio capture not working
See [SETUP.md](SETUP.md#step-4-configure-audio-capture) for platform-specific audio configuration.

### Out of memory
Use smaller models:
```python
stt_model_path = "tiny"      # Smaller Whisper
llm_model_name = "qwen3:1.7b"  # Already smallest recommended
```

### Slow performance
- Use GPU for Whisper (auto-detected if available)
- Use smaller models
- Increase summary interval: `summary_interval = 600`

## ğŸ“š Documentation

- [SETUP.md](SETUP.md) - **Quick setup guide** (recommended)
- [ARCHITECTURE.md](ARCHITECTURE.md) - System design and components
- [DATA_FLOW.md](DATA_FLOW.md) - Privacy architecture and data lifecycle
- [INSTALL.md](INSTALL.md) - Detailed platform-specific installation
- [INSTALL_UV.md](INSTALL_UV.md) - Installation using uv
- [UV_MIGRATION.md](UV_MIGRATION.md) - Migration from pip to uv

## ğŸ› ï¸ Development

### Install with dev tools:
```bash
uv sync --extra dev
```

### Run tests:
```bash
pytest
```

### Format code:
```bash
black .
ruff check .
```

## ğŸ’¾ What Gets Saved?

| Data Type | Location | Saved? |
|-----------|----------|--------|
| Raw Audio | Audio Queue | âŒ No |
| Audio Buffer | STT Component | âŒ No |
| Transcripts | Transcript Buffer | âŒ No |
| Intermediate Summaries | Summarizer (RAM) | âŒ No |
| **Final Summary** | **./summaries/** | **âœ… Yes** |

Only final summaries (2-5 KB text files) are saved to disk.

## ğŸ” Security & Privacy

- ğŸ”’ **100% Local Processing** - No data leaves your computer
- ğŸ”’ **No Cloud APIs** - Everything runs on-device
- ğŸ”’ **Minimal Disk Usage** - Only summaries saved
- ğŸ”’ **Open Source** - Inspect the code yourself
- ğŸ”’ **No Telemetry** - No tracking or analytics
- ğŸ”’ **HIPAA Friendly** - Suitable for confidential discussions

## ğŸ¤ Contributing

Contributions welcome! This is a privacy-first project focused on:
- Local-first processing
- Minimal disk persistence
- User control and transparency

## ğŸ“„ License

MIT License - See LICENSE file for details

## ğŸ™ Acknowledgments

Built with:
- [OpenAI Whisper](https://github.com/openai/whisper) - Speech recognition
- [Ollama](https://ollama.com/) - Local LLM inference
- [Qwen3](https://ollama.com/library/qwen3) - Efficient language model
- [uv](https://github.com/astral-sh/uv) - Fast Python package manager

---

**Get started in 5 minutes:** See [SETUP.md](SETUP.md)
